{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import exp, array, random, dot\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \n",
    "    def __init__(self, layer_dimensions, drop_prob = 0.0,reg_lambda = 0.0):\n",
    "        np.random.seed(1)\n",
    "        self.parameters = {}\n",
    "        self.layer_dimensions = layer_dimensions\n",
    "        self.num_layers = len(layer_dimensions) - 1\n",
    "        self.drop_prob = drop_prob\n",
    "        self.reg_lambda = reg_lambda\n",
    "        #self.biases = [np.random.randn(y, 1) for y in layer_dimensions[1:]]\n",
    "        #self.weights = [np.random.randn(y, x) for x, y in zip(layer_dimensions[:-1], layer_dimensions[1:])]\n",
    "        for l in range(1,len(layer_dimensions)):\n",
    "            self.parameters['W' + str(l)] = np.random.randn(layer_dimensions[l],layer_dimensions[l-1])*0.01 + .01\n",
    "            self.parameters['b' + str(l)] = np.zeros((layer_dimensions[l],1))\n",
    "    \n",
    "    def affineForward(self, A, W, b):\n",
    "        z = np.dot(W,A)+b\n",
    "        if np.isnan(W[0][0]):\n",
    "            print(\"w = \",W)\n",
    "            print(\"A=\",A)\n",
    "        cache = (A,W,b,z)\n",
    "        return z,cache\n",
    "    \n",
    "    def activationForward(self, A, activation='relu'):\n",
    "        if activation==\"relu\":\n",
    "            return self.relu(A)\n",
    "        elif activation==\"softmax\":\n",
    "            temp = self.softmax(A)\n",
    "            if temp.size==0:\n",
    "                print(A)\n",
    "            else:\n",
    "                return temp\n",
    "    \n",
    "    def relu(self, X):\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def forwardPropagation(self, X):\n",
    "        caches = []\n",
    "        z = X\n",
    "        layer_no = 0\n",
    "        L = len(self.parameters) // 2\n",
    "        for l in range(1,L):\n",
    "            z, cache = self.affineForward(z, self.parameters['W' + str(l)],self.parameters['b' + str(l)])\n",
    "            A = self.activationForward(z, activation=\"relu\")\n",
    "            caches.append(cache)\n",
    "        \n",
    "        ZL, cache_final_layer = self.affineForward(A, self.parameters['W' + str(L)],self.parameters['b' + str(L)])\n",
    "        AL = self.activationForward(ZL, activation=\"softmax\")\n",
    "\n",
    "        caches.append(cache_final_layer)\n",
    "        return AL, caches\n",
    "    \n",
    "    def costFunction(self, AL, y):\n",
    "        L = AL\n",
    "        shape = y.shape[0]\n",
    "        correct_label_prob = L[y,range(shape)]\n",
    "        cost = - np.sum(np.log(correct_label_prob))/shape\n",
    "        cost = np.squeeze(cost)\n",
    "        if(self.reg_lambda > 0):\n",
    "            pass\n",
    "        dAL = self.softmax_prime(AL,y)\n",
    "        return cost,dAL\n",
    "    \n",
    "    def affineBackward(self, dA_prev, cache):\n",
    "        A_prev,W,b,Z = cache\n",
    "        m = A_prev.shape[1]\n",
    "        dW = np.dot(dA_prev,A_prev.T)/m\n",
    "        db = np.sum(dA_prev,axis=1, keepdims=True)/m\n",
    "        dA = np.dot(W.T,dA_prev)\n",
    "        return dA, dW, db\n",
    "    \n",
    "    def activationBackward(self, dA, cache, activation='relu'):\n",
    "        A_prev,W,b,Z = cache\n",
    "        if activation==\"relu\":\n",
    "            return self.relu_derivative(dA,Z)\n",
    "    \n",
    "    def relu_derivative(self, dX, cached_x):\n",
    "        dX[cached_x < 0] = 0\n",
    "        dX[cached_x >= 0] = 1\n",
    "        return dX\n",
    "    \n",
    "    def backPropagation(self, dAL, y, cache):\n",
    "        gradients = {}\n",
    "        L = len(cache)\n",
    "        m = dAL.shape[1]\n",
    "        current_cache = cache[L-1]\n",
    "        gradients[\"dA\" + str(L)], gradients[\"dW\" + str(L)], gradients[\"db\" + str(L)] = \\\n",
    "        self.affineBackward(dAL,current_cache)\n",
    "        \n",
    "        for l in reversed(range(L-1)):\n",
    "            current_cache = cache[l]\n",
    "            dA_prev_non_linear = self.activationBackward(gradients[\"dA\" + str(l + 2)],current_cache,\"relu\")\n",
    "            dA_prev, dW_temp, db_temp = self.affineBackward(dA_prev_non_linear,current_cache)\n",
    "            gradients[\"dA\" + str(l + 1)] = dA_prev\n",
    "            gradients[\"dW\" + str(l + 1)] = dW_temp\n",
    "            gradients[\"db\" + str(l + 1)] = db_temp\n",
    "            \n",
    "        return gradients\n",
    "    \n",
    "    def updateParameters(self, gradients, alpha):\n",
    "        L = len(self.parameters) // 2\n",
    "        for l in range(L):\n",
    "            self.parameters[\"W\" + str(l+1)] = self.parameters[\"W\" + str(l+1)] - alpha*gradients[\"dW\" + str(l+1)]\n",
    "            self.parameters[\"b\" + str(l+1)] = self.parameters[\"b\" + str(l+1)] - alpha*gradients[\"db\" + str(l+1)]\n",
    "    \n",
    "    def get_batch(self, X, y, batch_size):\n",
    "        current_index=self.parameters[\"batch_index\"]\n",
    "        self.parameters[\"batch_index\"]=self.parameters[\"batch_index\"]+batch_size\n",
    "        X_batch,y_batch = X[:,current_index:current_index+batch_size], y[current_index:current_index+batch_size]\n",
    "\n",
    "        return X_batch, y_batch\n",
    "    \n",
    "    def train(self, X, y, iters=100, alpha=0.1, batch_size=100, print_every=10):\n",
    "        batch_iters = (int)(X.shape[1]/batch_size)\n",
    "        cost = 0\n",
    "        for i in range(0, iters):\n",
    "            self.parameters[\"batch_index\"] = 0\n",
    "            for j in range(batch_iters):\n",
    "                X_batch,Y_batch=self.get_batch( X, y, batch_size)\n",
    "                AL, all_layer_cache = self.forwardPropagation(X_batch)\n",
    "                cost, dAL = self.costFunction(AL, Y_batch)\n",
    "                gradients = self.backPropagation(dAL, Y_batch, all_layer_cache)\n",
    "                self.updateParameters(gradients, alpha)\n",
    "            if i % print_every == 0:\n",
    "                print \"Cost: \", cost\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred, cache = self.forwardPropagation(X)\n",
    "        y_pred = self.softmax(y_pred)\n",
    "        predicted_labels=[]\n",
    "        for i in range(y_pred.shape[1]):\n",
    "            label_predicted=max(y_pred[:,i])\n",
    "            predicted_labels.append(y_pred[:,i].tolist().index(label_predicted))\n",
    "        predicted_labels = np.array(predicted_labels)\n",
    "        return predicted_labels\n",
    "    \n",
    "    def softmax(self, X):\n",
    "        return np.exp(X)/ np.sum(np.exp(X), axis = 0)\n",
    "    \n",
    "    def softmax_prime(self, X, y):\n",
    "        return X - self.one_hot_encoding(y)\n",
    "    \n",
    "    def one_hot_encoding(self, x, n_classes = 10):\n",
    "        one_hot = np.zeros((n_classes, x.shape[0]))\n",
    "        one_hot[x, range(x.shape[0])] = 1\n",
    "        return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.09999595  0.09999595]\n",
      " [ 0.0999962   0.0999962 ]\n",
      " [ 0.10000244  0.10000244]\n",
      " [ 0.100002    0.100002  ]\n",
      " [ 0.10000016  0.10000016]\n",
      " [ 0.09999968  0.09999968]\n",
      " [ 0.10000376  0.10000376]\n",
      " [ 0.09999776  0.09999776]\n",
      " [ 0.09999943  0.09999943]\n",
      " [ 0.10000262  0.10000262]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sum() got an unexpected keyword argument 'keepdims'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-254-3b0d91520581>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_layer_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforwardPropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdAL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcostFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackPropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_layer_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-253-00ac02963c9e>\u001b[0m in \u001b[0;36mbackPropagation\u001b[0;34m(self, dAL, y, cache)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdAL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mcurrent_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dA\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dW\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"db\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffineBackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdAL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrent_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-253-00ac02963c9e>\u001b[0m in \u001b[0;36maffineBackward\u001b[0;34m(self, dA_prev, cache)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA_prev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA_prev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mdA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdA_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rakshitsareen/anaconda2/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   1830\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1831\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1832\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1833\u001b[0m     return _methods._sum(a, axis=axis, dtype=dtype,\n\u001b[1;32m   1834\u001b[0m                          out=out, **kwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: sum() got an unexpected keyword argument 'keepdims'"
     ]
    }
   ],
   "source": [
    "NN = NeuralNetwork([5,30,20,10])\n",
    "X = np.matrix([[1,1],[1,1],[1,1],[1,1],[1,1]])\n",
    "last, cache = NN.forwardPropagation(X)\n",
    "print last\n",
    "Y = np.matrix([[1],[0]])\n",
    "AL, all_layer_cache = NN.forwardPropagation(X)\n",
    "cost,dAL = NN.costFunction(AL, Y)\n",
    "gradients=NN.backPropagation(dAL, Y, all_layer_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:  0\n",
      "Cost:  0\n",
      "Cost:  0\n",
      "Cost:  0\n",
      "Cost:  0\n",
      "Cost:  0\n",
      "Cost:  0\n",
      "Cost:  0\n",
      "Cost:  0\n",
      "Cost:  0\n"
     ]
    }
   ],
   "source": [
    "NN.train(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 32, 32)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0][0].numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_output_from_int(a):\n",
    "    out = np.zeros((10,1))\n",
    "    out[a ,:] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_numpy_from_CIFAR10_data(data, no_of_samples):\n",
    "    out = []\n",
    "    inp = np.zeros((3072, no_of_samples))\n",
    "    y = []\n",
    "    for tensor in range(no_of_samples):\n",
    "        inp[:, tensor] = data[tensor][0].view(3072, -1).numpy()[:,0]\n",
    "        y.append(int((data[tensor])[1]))\n",
    "    out = np.matrix(y)\n",
    "    out = out.reshape(out.shape[1],out.shape[0])\n",
    "    return inp, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train= make_numpy_from_CIFAR10_data(trainset, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train= make_numpy_from_CIFAR10_data(trainset, 50000)\n",
    "X_validate = x_train[:,45001:50000]\n",
    "Y_validate = y_train[45001:50000]\n",
    "\n",
    "X_train_temp = x_train[:,:45000]\n",
    "Y_train_temp = y_train[:45000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:  2.38487664436\n",
      "Cost:  2.38487664436\n",
      "Cost:  2.38487664436\n",
      "Cost:  2.38487664436\n",
      "Cost:  2.38487664436\n"
     ]
    }
   ],
   "source": [
    "NN = NeuralNetwork([3072,32,16,10],drop_prob=0.2, reg_lambda=0.1)\n",
    "NN.train(X_train_temp, Y_train_temp,iters=5, alpha=.01, batch_size=1, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4999,)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_validate = NN.predict(X_validate)\n",
    "print(y_predicted_validate)\n",
    "len(y_predicted_validate)\n",
    "y_predicted_validate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-283-1af98f59afd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_validate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'predicted_labels' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(predicted_labels,Y_validate)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
